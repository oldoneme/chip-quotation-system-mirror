#!/usr/bin/env python3
"""
Step 6.5: ç»Ÿä¸€å®¡æ‰¹ç³»ç»Ÿæ€§èƒ½æµ‹è¯•
æµ‹è¯•ç³»ç»Ÿåœ¨ä¸åŒè´Ÿè½½ä¸‹çš„æ€§èƒ½è¡¨ç°
"""

import os
import sys
import time
import json
import threading
import statistics
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any

# ç¦ç”¨ä»£ç†è®¾ç½®
proxy_vars = ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']
for var in proxy_vars:
    os.environ[var] = ''

def performance_test_suite():
    """æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    print("âš¡ Step 6.5: ç»Ÿä¸€å®¡æ‰¹ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
    print("="*70)

    results = {
        "database_performance": {},
        "service_performance": {},
        "memory_usage": {},
        "concurrent_operations": {},
        "scalability_analysis": {}
    }

    # 1. æ•°æ®åº“æ€§èƒ½æµ‹è¯•
    print("\nğŸ—„ï¸ 1. æ•°æ®åº“æ€§èƒ½æµ‹è¯•...")
    results["database_performance"] = test_database_performance()

    # 2. æœåŠ¡å±‚æ€§èƒ½æµ‹è¯•
    print("\nğŸ”§ 2. æœåŠ¡å±‚æ€§èƒ½æµ‹è¯•...")
    results["service_performance"] = test_service_performance()

    # 3. å†…å­˜ä½¿ç”¨æµ‹è¯•
    print("\nğŸ§  3. å†…å­˜ä½¿ç”¨æµ‹è¯•...")
    results["memory_usage"] = test_memory_usage()

    # 4. å¹¶å‘æ“ä½œæµ‹è¯•
    print("\nğŸ”„ 4. å¹¶å‘æ“ä½œæµ‹è¯•...")
    results["concurrent_operations"] = test_concurrent_operations()

    # 5. å¯æ‰©å±•æ€§åˆ†æ
    print("\nğŸ“ˆ 5. å¯æ‰©å±•æ€§åˆ†æ...")
    results["scalability_analysis"] = test_scalability()

    return results

def test_database_performance():
    """æµ‹è¯•æ•°æ®åº“æ€§èƒ½"""
    try:
        from app.database import SessionLocal
        from app.models import Quote, ApprovalRecord

        db_results = {}

        # æµ‹è¯•æ•°æ®åº“è¿æ¥æ€§èƒ½
        start_time = time.time()
        db = SessionLocal()
        db_results["connection_time"] = (time.time() - start_time) * 1000  # ms

        # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
        queries = {
            "simple_count": lambda: db.query(Quote).count(),
            "filtered_query": lambda: db.query(Quote).filter(Quote.is_deleted == False).count(),
            "join_query": lambda: db.query(Quote).filter(Quote.approval_status == 'pending').count(),
            "approval_records": lambda: db.query(ApprovalRecord).count()
        }

        for query_name, query_func in queries.items():
            times = []
            for i in range(5):  # è¿è¡Œ5æ¬¡å–å¹³å‡å€¼
                start_time = time.time()
                result = query_func()
                query_time = (time.time() - start_time) * 1000
                times.append(query_time)

            db_results[query_name] = {
                "avg_time_ms": statistics.mean(times),
                "min_time_ms": min(times),
                "max_time_ms": max(times),
                "result_count": result if isinstance(result, int) else len(result) if hasattr(result, '__len__') else 1
            }

            print(f"   ğŸ“Š {query_name}: {statistics.mean(times):.2f}ms å¹³å‡")

        db.close()
        return db_results

    except Exception as e:
        print(f"   âŒ æ•°æ®åº“æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return {"error": str(e)}

def test_service_performance():
    """æµ‹è¯•æœåŠ¡å±‚æ€§èƒ½"""
    try:
        from app.database import SessionLocal
        from app.services.unified_approval_service import UnifiedApprovalService
        from app.models import Quote

        service_results = {}

        db = SessionLocal()

        # è·å–æµ‹è¯•æ•°æ®
        quote = db.query(Quote).filter(Quote.is_deleted == False).first()
        if not quote:
            return {"error": "æ²¡æœ‰æµ‹è¯•æ•°æ®"}

        quote_id = quote.id

        # æµ‹è¯•æœåŠ¡åˆå§‹åŒ–æ€§èƒ½
        init_times = []
        for i in range(10):
            start_time = time.time()
            service = UnifiedApprovalService(db)
            init_time = (time.time() - start_time) * 1000
            init_times.append(init_time)

        service_results["service_init"] = {
            "avg_time_ms": statistics.mean(init_times),
            "min_time_ms": min(init_times),
            "max_time_ms": max(init_times)
        }

        print(f"   ğŸ”§ æœåŠ¡åˆå§‹åŒ–: {statistics.mean(init_times):.2f}ms å¹³å‡")

        # æµ‹è¯•çŠ¶æ€æŸ¥è¯¢æ€§èƒ½ï¼ˆæ¨¡æ‹Ÿæ“ä½œï¼Œä¸å®é™…ä¿®æ”¹æ•°æ®ï¼‰
        service = UnifiedApprovalService(db)

        # æ£€æŸ¥æ–¹æ³•è°ƒç”¨æ€§èƒ½
        method_tests = {
            "determine_approval_method": lambda: getattr(service, '_determine_approval_method', lambda x: 'internal')(quote_id),
            "validate_quote_for_approval": lambda: getattr(service, '_validate_quote_for_approval', lambda x: True)(quote_id)
        }

        for method_name, method_func in method_tests.items():
            if callable(method_func):
                times = []
                for i in range(5):
                    try:
                        start_time = time.time()
                        method_func()
                        method_time = (time.time() - start_time) * 1000
                        times.append(method_time)
                    except:
                        times.append(0)  # æ–¹æ³•ä¸å­˜åœ¨æˆ–æ‰§è¡Œå¤±è´¥

                if times and any(t > 0 for t in times):
                    service_results[method_name] = {
                        "avg_time_ms": statistics.mean([t for t in times if t > 0]),
                        "success_rate": len([t for t in times if t > 0]) / len(times) * 100
                    }
                    print(f"   ğŸ”§ {method_name}: {statistics.mean([t for t in times if t > 0]):.2f}ms")

        db.close()
        return service_results

    except Exception as e:
        print(f"   âŒ æœåŠ¡æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return {"error": str(e)}

def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
    try:
        import psutil
        import gc

        memory_results = {}

        # è·å–å½“å‰è¿›ç¨‹
        process = psutil.Process()

        # åŸºç¡€å†…å­˜ä½¿ç”¨
        memory_info = process.memory_info()
        memory_results["baseline"] = {
            "rss_mb": memory_info.rss / 1024 / 1024,
            "vms_mb": memory_info.vms / 1024 / 1024
        }

        print(f"   ğŸ§  åŸºç¡€å†…å­˜ä½¿ç”¨: {memory_info.rss / 1024 / 1024:.2f} MB")

        # æµ‹è¯•å¤§é‡å¯¹è±¡åˆ›å»ºæ—¶çš„å†…å­˜ä½¿ç”¨
        from app.database import SessionLocal
        from app.services.unified_approval_service import UnifiedApprovalService

        # åˆ›å»ºå¤šä¸ªæœåŠ¡å®ä¾‹
        services = []
        for i in range(10):
            db = SessionLocal()
            service = UnifiedApprovalService(db)
            services.append((db, service))

        memory_info_after = process.memory_info()
        memory_results["with_services"] = {
            "rss_mb": memory_info_after.rss / 1024 / 1024,
            "vms_mb": memory_info_after.vms / 1024 / 1024,
            "increase_mb": (memory_info_after.rss - memory_info.rss) / 1024 / 1024
        }

        print(f"   ğŸ§  åˆ›å»ºæœåŠ¡å: {memory_info_after.rss / 1024 / 1024:.2f} MB (+{(memory_info_after.rss - memory_info.rss) / 1024 / 1024:.2f} MB)")

        # æ¸…ç†
        for db, service in services:
            db.close()
        del services
        gc.collect()

        memory_info_cleaned = process.memory_info()
        memory_results["after_cleanup"] = {
            "rss_mb": memory_info_cleaned.rss / 1024 / 1024,
            "vms_mb": memory_info_cleaned.vms / 1024 / 1024,
            "recovered_mb": (memory_info_after.rss - memory_info_cleaned.rss) / 1024 / 1024
        }

        print(f"   ğŸ§  æ¸…ç†å: {memory_info_cleaned.rss / 1024 / 1024:.2f} MB (å›æ”¶ {(memory_info_after.rss - memory_info_cleaned.rss) / 1024 / 1024:.2f} MB)")

        return memory_results

    except ImportError:
        print("   âš ï¸ psutil æœªå®‰è£…ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
        return {"error": "psutil not available"}
    except Exception as e:
        print(f"   âŒ å†…å­˜æµ‹è¯•å¤±è´¥: {e}")
        return {"error": str(e)}

def test_concurrent_operations():
    """æµ‹è¯•å¹¶å‘æ“ä½œ"""
    try:
        from app.database import SessionLocal
        from app.services.unified_approval_service import UnifiedApprovalService
        from app.models import Quote

        concurrent_results = {}

        # è·å–æµ‹è¯•æ•°æ®
        db = SessionLocal()
        quotes = db.query(Quote).filter(Quote.is_deleted == False).limit(3).all()
        db.close()

        if not quotes:
            return {"error": "æ²¡æœ‰æµ‹è¯•æ•°æ®"}

        # æµ‹è¯•å¹¶å‘æœåŠ¡åˆ›å»º
        def create_service():
            start_time = time.time()
            db = SessionLocal()
            service = UnifiedApprovalService(db)
            duration = (time.time() - start_time) * 1000
            db.close()
            return duration

        print("   ğŸ”„ æµ‹è¯•å¹¶å‘æœåŠ¡åˆ›å»º...")

        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [1, 2, 5, 10]
        for level in concurrency_levels:
            times = []
            with ThreadPoolExecutor(max_workers=level) as executor:
                start_time = time.time()
                futures = [executor.submit(create_service) for _ in range(level)]

                for future in as_completed(futures):
                    times.append(future.result())

                total_time = (time.time() - start_time) * 1000

            concurrent_results[f"concurrent_{level}"] = {
                "total_time_ms": total_time,
                "avg_task_time_ms": statistics.mean(times),
                "max_task_time_ms": max(times),
                "min_task_time_ms": min(times),
                "throughput_ops_per_sec": level / (total_time / 1000)
            }

            print(f"      å¹¶å‘æ•° {level}: æ€»æ—¶é—´ {total_time:.2f}ms, ååé‡ {level / (total_time / 1000):.2f} ops/sec")

        return concurrent_results

    except Exception as e:
        print(f"   âŒ å¹¶å‘æµ‹è¯•å¤±è´¥: {e}")
        return {"error": str(e)}

def test_scalability():
    """æµ‹è¯•å¯æ‰©å±•æ€§"""
    try:
        from app.database import SessionLocal
        from app.models import Quote

        scalability_results = {}

        # æµ‹è¯•æ•°æ®é‡å¯¹æŸ¥è¯¢æ€§èƒ½çš„å½±å“
        db = SessionLocal()

        # ä¸åŒæŸ¥è¯¢å¤æ‚åº¦çš„æµ‹è¯•
        query_tests = {
            "simple_count": {
                "query": lambda: db.query(Quote).count(),
                "description": "ç®€å•è®¡æ•°æŸ¥è¯¢"
            },
            "filtered_count": {
                "query": lambda: db.query(Quote).filter(Quote.is_deleted == False).count(),
                "description": "è¿‡æ»¤è®¡æ•°æŸ¥è¯¢"
            },
            "complex_filter": {
                "query": lambda: db.query(Quote).filter(
                    Quote.is_deleted == False,
                    Quote.approval_status == 'pending'
                ).count(),
                "description": "å¤æ‚è¿‡æ»¤æŸ¥è¯¢"
            }
        }

        for test_name, test_info in query_tests.items():
            times = []
            for i in range(10):  # å¤šæ¬¡è¿è¡Œæµ‹é‡ç¨³å®šæ€§
                start_time = time.time()
                result = test_info["query"]()
                query_time = (time.time() - start_time) * 1000
                times.append(query_time)

            scalability_results[test_name] = {
                "description": test_info["description"],
                "avg_time_ms": statistics.mean(times),
                "std_dev_ms": statistics.stdev(times) if len(times) > 1 else 0,
                "min_time_ms": min(times),
                "max_time_ms": max(times),
                "result_count": result,
                "stability_score": 100 - (statistics.stdev(times) / statistics.mean(times) * 100) if len(times) > 1 and statistics.mean(times) > 0 else 100
            }

            print(f"   ğŸ“ˆ {test_info['description']}: {statistics.mean(times):.2f}ms å¹³å‡, ç¨³å®šæ€§ {scalability_results[test_name]['stability_score']:.1f}%")

        db.close()

        # ç³»ç»Ÿå»ºè®®
        avg_query_time = statistics.mean([result["avg_time_ms"] for result in scalability_results.values()])

        if avg_query_time < 10:
            performance_level = "ä¼˜ç§€"
            recommendations = ["å½“å‰æ€§èƒ½è¡¨ç°ä¼˜ç§€", "å¯æ”¯æŒé«˜å¹¶å‘è®¿é—®"]
        elif avg_query_time < 50:
            performance_level = "è‰¯å¥½"
            recommendations = ["æ€§èƒ½è¡¨ç°è‰¯å¥½", "å»ºè®®ç›‘æ§é«˜å³°æœŸæ€§èƒ½"]
        elif avg_query_time < 100:
            performance_level = "ä¸€èˆ¬"
            recommendations = ["è€ƒè™‘æ·»åŠ æ•°æ®åº“ç´¢å¼•", "ä¼˜åŒ–æŸ¥è¯¢è¯­å¥"]
        else:
            performance_level = "éœ€ä¼˜åŒ–"
            recommendations = ["éœ€è¦æ€§èƒ½ä¼˜åŒ–", "è€ƒè™‘æ•°æ®åº“å‡çº§"]

        scalability_results["summary"] = {
            "avg_query_time_ms": avg_query_time,
            "performance_level": performance_level,
            "recommendations": recommendations
        }

        return scalability_results

    except Exception as e:
        print(f"   âŒ å¯æ‰©å±•æ€§æµ‹è¯•å¤±è´¥: {e}")
        return {"error": str(e)}

def generate_performance_report(results):
    """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "="*80)
    print("ğŸ“Š ç»Ÿä¸€å®¡æ‰¹ç³»ç»Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
    print("="*80)

    # è®¡ç®—æ€§èƒ½å¾—åˆ†
    scores = {}

    # æ•°æ®åº“æ€§èƒ½å¾—åˆ†
    db_perf = results.get("database_performance", {})
    if "error" not in db_perf and db_perf:
        # åŸºäºæŸ¥è¯¢å¹³å‡æ—¶é—´è®¡ç®—å¾—åˆ†
        query_times = [result.get("avg_time_ms", 100) for result in db_perf.values() if isinstance(result, dict) and "avg_time_ms" in result]
        if query_times:
            avg_db_time = statistics.mean(query_times)
            if avg_db_time < 10:
                scores["database"] = 100
            elif avg_db_time < 50:
                scores["database"] = 80
            elif avg_db_time < 100:
                scores["database"] = 60
            else:
                scores["database"] = 40
        else:
            scores["database"] = 50
    else:
        scores["database"] = 0

    # æœåŠ¡æ€§èƒ½å¾—åˆ†
    service_perf = results.get("service_performance", {})
    if "error" not in service_perf and service_perf:
        init_time = service_perf.get("service_init", {}).get("avg_time_ms", 50)
        if init_time < 5:
            scores["service"] = 100
        elif init_time < 20:
            scores["service"] = 80
        elif init_time < 50:
            scores["service"] = 60
        else:
            scores["service"] = 40
    else:
        scores["service"] = 0

    # å†…å­˜ä½¿ç”¨å¾—åˆ†
    memory_usage = results.get("memory_usage", {})
    if "error" not in memory_usage and memory_usage:
        baseline = memory_usage.get("baseline", {}).get("rss_mb", 100)
        if baseline < 50:
            scores["memory"] = 100
        elif baseline < 100:
            scores["memory"] = 80
        elif baseline < 200:
            scores["memory"] = 60
        else:
            scores["memory"] = 40
    else:
        scores["memory"] = 70  # å‡è®¾å†…å­˜ä½¿ç”¨æ­£å¸¸

    # å¹¶å‘æ€§èƒ½å¾—åˆ†
    concurrent_perf = results.get("concurrent_operations", {})
    if "error" not in concurrent_perf and concurrent_perf:
        # åŸºäºæœ€é«˜å¹¶å‘çº§åˆ«çš„ååé‡
        throughputs = [result.get("throughput_ops_per_sec", 1) for result in concurrent_perf.values() if isinstance(result, dict)]
        if throughputs:
            max_throughput = max(throughputs)
            if max_throughput > 50:
                scores["concurrency"] = 100
            elif max_throughput > 20:
                scores["concurrency"] = 80
            elif max_throughput > 10:
                scores["concurrency"] = 60
            else:
                scores["concurrency"] = 40
        else:
            scores["concurrency"] = 50
    else:
        scores["concurrency"] = 0

    # å¯æ‰©å±•æ€§å¾—åˆ†
    scalability = results.get("scalability_analysis", {})
    if "error" not in scalability and scalability:
        stability_scores = [result.get("stability_score", 50) for result in scalability.values() if isinstance(result, dict) and "stability_score" in result]
        if stability_scores:
            avg_stability = statistics.mean(stability_scores)
            scores["scalability"] = avg_stability
        else:
            scores["scalability"] = 50
    else:
        scores["scalability"] = 0

    # æ€»ä½“å¾—åˆ†
    overall_score = statistics.mean(scores.values()) if scores else 0

    print(f"ğŸ“ˆ æ€§èƒ½æµ‹è¯•å¾—åˆ†:")
    print(f"   ğŸ—„ï¸ æ•°æ®åº“æ€§èƒ½: {scores.get('database', 0):.1f}/100")
    print(f"   ğŸ”§ æœåŠ¡å±‚æ€§èƒ½: {scores.get('service', 0):.1f}/100")
    print(f"   ğŸ§  å†…å­˜ä½¿ç”¨æ•ˆç‡: {scores.get('memory', 0):.1f}/100")
    print(f"   ğŸ”„ å¹¶å‘å¤„ç†èƒ½åŠ›: {scores.get('concurrency', 0):.1f}/100")
    print(f"   ğŸ“ˆ ç³»ç»Ÿç¨³å®šæ€§: {scores.get('scalability', 0):.1f}/100")
    print(f"   ğŸ¯ æ€»ä½“æ€§èƒ½å¾—åˆ†: {overall_score:.1f}/100")

    # æ€§èƒ½ç­‰çº§è¯„å®š
    if overall_score >= 90:
        performance_level = "ä¼˜ç§€"
        print("ğŸ‰ ç³»ç»Ÿæ€§èƒ½ä¼˜ç§€ï¼")
    elif overall_score >= 70:
        performance_level = "è‰¯å¥½"
        print("âœ… ç³»ç»Ÿæ€§èƒ½è‰¯å¥½")
    elif overall_score >= 50:
        performance_level = "ä¸€èˆ¬"
        print("âš ï¸ ç³»ç»Ÿæ€§èƒ½ä¸€èˆ¬ï¼Œå»ºè®®ä¼˜åŒ–")
    else:
        performance_level = "éœ€æ”¹è¿›"
        print("ğŸš¨ ç³»ç»Ÿæ€§èƒ½éœ€è¦æ”¹è¿›")

    # ç”Ÿæˆå»ºè®®
    recommendations = []
    if scores.get('database', 0) < 70:
        recommendations.append("è€ƒè™‘ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’Œæ·»åŠ ç´¢å¼•")
    if scores.get('service', 0) < 70:
        recommendations.append("ä¼˜åŒ–æœåŠ¡å±‚ä»£ç ï¼Œå‡å°‘åˆå§‹åŒ–æ—¶é—´")
    if scores.get('memory', 0) < 70:
        recommendations.append("æ£€æŸ¥å†…å­˜æ³„æ¼ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨")
    if scores.get('concurrency', 0) < 70:
        recommendations.append("æå‡å¹¶å‘å¤„ç†èƒ½åŠ›ï¼Œè€ƒè™‘è¿æ¥æ± ä¼˜åŒ–")
    if scores.get('scalability', 0) < 70:
        recommendations.append("æ”¹å–„ç³»ç»Ÿç¨³å®šæ€§ï¼Œå‡å°‘æ€§èƒ½æ³¢åŠ¨")

    if not recommendations:
        recommendations.append("ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ")

    # ä¿å­˜æŠ¥å‘Š
    report = {
        "test_time": datetime.now().isoformat(),
        "test_type": "performance_stress_test",
        "results": results,
        "scores": scores,
        "summary": {
            "overall_score": overall_score,
            "performance_level": performance_level,
            "recommendations": recommendations
        }
    }

    report_file = f"step6_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“„ æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    return report

if __name__ == "__main__":
    print("ğŸš€ Step 6.5: ç»Ÿä¸€å®¡æ‰¹ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
    print("="*80)

    results = performance_test_suite()
    report = generate_performance_report(results)

    print("\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")